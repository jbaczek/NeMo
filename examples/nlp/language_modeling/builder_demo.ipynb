{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86d41806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from typing import Any, Dict, Callable, Optional, Tuple, TypeVar, Union, List\n",
    "from dataclasses import dataclass\n",
    "from omegaconf import OmegaConf\n",
    "from hydra.core.config_store import ConfigStore\n",
    "from pprint import pprint\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e597bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from this thread: https://github.com/facebookresearch/hydra/issues/1982\n",
    "# This aims to add support for vscode autocompletion\n",
    "\n",
    "\n",
    "\n",
    "SUBCONFIG_PATHS = {\n",
    "    'optim' : 'optim@model.optim',\n",
    "    'tokenizer': 'model/tokenizer@model.tokenizer'\n",
    "}\n",
    "\n",
    "_T = TypeVar('_T')\n",
    "\n",
    "def __dataclass_transform__(\n",
    "    *,\n",
    "    eq_default: bool = True,\n",
    "    order_default: bool = False,\n",
    "    kw_only_default: bool = False,\n",
    "    field_descriptors: Tuple[Union[type, Callable[..., Any]], ...] = (()),\n",
    ") -> Callable[[_T], _T]:\n",
    "    return lambda a: a\n",
    "\n",
    "\n",
    "@__dataclass_transform__(eq_default=True, order_default=False, kw_only_default=True)\n",
    "def structured_config(\n",
    "    name: Optional[str] = None,\n",
    "    group: Optional[str] = None,\n",
    "    package: Optional[str] = None,\n",
    "    provider: Optional[str] = None,\n",
    "    init: bool = True,\n",
    "    repr: bool = True,\n",
    "    eq: bool = True,\n",
    "    order: bool = False,\n",
    "    unsafe_hash: bool = False,\n",
    "    frozen: bool = False\n",
    "):\n",
    "    def decorator(cls=None):\n",
    "        def wrapper(cls: Any):\n",
    "            # Wrap class into a dataclass\n",
    "            new_cls = dataclass(cls, init=init, repr=repr, eq=eq, order=order, unsafe_hash=unsafe_hash, frozen=frozen)            \n",
    "            # Store structure config into Config Store\n",
    "            if name is not None:\n",
    "                config_store = ConfigStore.instance()\n",
    "                config_store.store(group=group, name=name, package=package, provider=provider, node=new_cls)\n",
    "             \n",
    "            #### WIP: Configs without a name append their fields instead of substituting whole subtree\n",
    "            if name:\n",
    "                new_cls.hydra_override_str = f'{SUBCONFIG_PATHS[group]}={name}'\n",
    "            else:\n",
    "                ov_str = SUBCONFIG_PATHS[group].split('@')[-1]\n",
    "                new_cls.hydra_override_str = ov_str\n",
    "            \n",
    "            #####\n",
    "            return new_cls\n",
    "        \n",
    "\n",
    "        # See if we're being called as @structured_config or @structured_config().\n",
    "        if cls is None:\n",
    "            return wrapper\n",
    "        return wrapper(cls)\n",
    "    return decorator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02d1dfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Singleton(type):\n",
    "    _instances = {}\n",
    "    def __call__(cls, *args, **kwargs):\n",
    "        if cls not in cls._instances:\n",
    "            cls._instances[cls] = super().__call__(*args, **kwargs)\n",
    "        return cls._instances[cls]\n",
    "        \n",
    "# We are making it a singleton because I still don't know how to handle multiple config dirs and this is a safe option\n",
    "class ConfigBuilder(metaclass=Singleton):\n",
    "    def __init__(self, config_dir):\n",
    "        if not GlobalHydra().is_initialized():\n",
    "            hydra.initialize_config_dir(config_dir=config_dir)\n",
    "        self._hydra = GlobalHydra().hydra\n",
    "        self.overrides = []\n",
    "    \n",
    "    def compose(self, task):\n",
    "        return hydra.compose(config_name=f'{task}_config', overrides=self.overrides)\n",
    "    \n",
    "    def str_override(self, s):\n",
    "        # TODO: Add some check whether this kind of override is already present\n",
    "        # The order of overrides is last-one-wins. So when we type model=gpt model=bert\n",
    "        # then the model ends up being bert\n",
    "        \n",
    "        # TODO: do a grammar check\n",
    "        self.overrides.append(s)\n",
    "        \n",
    "    def info(self, task, option='all'):\n",
    "        #options = {\n",
    "        #    \"all\": self._hydra._print_all_info,\n",
    "        #    \"defaults\": self._hydra._print_defaults_list,\n",
    "        #    \"defaults-tree\": self._hydra._print_defaults_tree,\n",
    "        #    \"config\": self._hydra._print_config_info,\n",
    "        #    \"plugins\": self._hydra._print_plugins_info,\n",
    "        #    \"searchpath\": self._hydra._print_search_path,\n",
    "        #}\n",
    "\n",
    "        GlobalHydra().hydra.show_info(option, config_name=f'{task}_config', overrides=self.overrides)\n",
    "        \n",
    "        # Clean up the logger handles. WAR needed, because hydra leaves them open\n",
    "        root = logging.getLogger()\n",
    "        for h in root.handlers:\n",
    "            h.close()\n",
    "        root.handlers=[]\n",
    "        \n",
    "    def override(self, cfg):  \n",
    "        # Dirty but works, needs rethinking!\n",
    "        if '=' not in cfg.hydra_override_str:\n",
    "            for k, v in vars(cfg()).items():\n",
    "                self.str_override(f'{cfg.hydra_override_str}.{k}={v}')\n",
    "        else:\n",
    "            self.str_override(cfg.hydra_override_str)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b862a2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@structured_config(group='optim', name='sgd')\n",
    "class SGDConfig:\n",
    "    lr: float = 1e-3\n",
    "    momentum: float = 0.0\n",
    "    dampening: float = 0.0\n",
    "    weight_decay: float = 0.0\n",
    "        \n",
    "@structured_config(group='optim')\n",
    "class MyOptim:\n",
    "    lr: float = 2e-2\n",
    "    weight_decay: float = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2c30320",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SDGConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mSDGConfig\u001b[49m(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SDGConfig' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73bcf3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = ConfigBuilder('/opt/NeMo/examples/nlp/language_modeling/alt_conf/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ceb4768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Defaults List\n",
      "*************\n",
      "| Config path                           | Package             | _self_ | Parent             | \n",
      "----------------------------------------------------------------------------------------------\n",
      "| hydra/output/default                  | hydra               | False  | hydra/config       |\n",
      "| hydra/launcher/basic                  | hydra.launcher      | False  | hydra/config       |\n",
      "| hydra/sweeper/basic                   | hydra.sweeper       | False  | hydra/config       |\n",
      "| hydra/help/default                    | hydra.help          | False  | hydra/config       |\n",
      "| hydra/hydra_help/default              | hydra.hydra_help    | False  | hydra/config       |\n",
      "| hydra/hydra_logging/default           | hydra.hydra_logging | False  | hydra/config       |\n",
      "| hydra/job_logging/default             | hydra.job_logging   | False  | hydra/config       |\n",
      "| hydra/env/default                     | hydra.env           | False  | hydra/config       |\n",
      "| hydra/config                          | hydra               | True   | <root>             |\n",
      "| trainer/megatron                      | trainer             | False  | pretraining_config |\n",
      "| exp_manager/megatron                  | exp_manager         | False  | pretraining_config |\n",
      "| lr_scheduler/cosine_annealing         | model.optim.sched   | False  | pretraining_config |\n",
      "| model/gpt                             | model               | True   | pretraining_config |\n",
      "| model/tokenizer/gpt_tokenizer         | model.tokenizer     | False  | model/gpt          |\n",
      "| model/mixins/transformer_engine       | model               | False  | model/gpt          |\n",
      "| model/mixins/fusion                   | model               | False  | model/gpt          |\n",
      "| model/mixins/mixed_precision          | model               | False  | model/gpt          |\n",
      "| model/mixins/nsys_profile             | model.nsys_profile  | False  | model/gpt          |\n",
      "| model/mixins/activation_checkpointing | model               | False  | model/gpt          |\n",
      "| optim/sgd                             | model.optim         | False  | pretraining_config |\n",
      "| data/gpt_data                         | model.data          | False  | pretraining_config |\n",
      "| model_overrides/gpt                   |                     | False  | pretraining_config |\n",
      "| pretraining_config                    |                     | True   | <root>             |\n",
      "----------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "builder.str_override('model=gpt')\n",
    "builder.override(SGDConfig) # Substitution\n",
    "builder.override(MyOptim) # Merging\n",
    "\n",
    "builder.info(task='pretraining', option='defaults')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d95b202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exp_manager': {'checkpoint_callback_params': {'always_save_nemo': False,\n",
      "                                                'filename': '${name}--{val_loss:.2f}-{step}-{consumed_samples}',\n",
      "                                                'mode': 'min',\n",
      "                                                'model_parallel_size': '${multiply:${model.tensor_model_parallel_size}, '\n",
      "                                                                       '${model.pipeline_model_parallel_size}}',\n",
      "                                                'monitor': 'val_loss',\n",
      "                                                'save_nemo_on_train_end': False,\n",
      "                                                'save_top_k': 10},\n",
      "                 'create_checkpoint_callback': True,\n",
      "                 'create_wandb_logger': False,\n",
      "                 'exp_dir': None,\n",
      "                 'explicit_log_dir': None,\n",
      "                 'name': '${name}',\n",
      "                 'resume_if_exists': True,\n",
      "                 'resume_ignore_no_checkpoint': True,\n",
      "                 'wandb_logger_kwargs': {'name': None, 'project': None}},\n",
      " 'model': {'_target_': 'nemo.collections.nlp.models.language_modeling.megatron_gpt_model.MegatronGPTModel',\n",
      "           'activation': 'gelu',\n",
      "           'activations_checkpoint_granularity': None,\n",
      "           'activations_checkpoint_layers_per_pipeline': None,\n",
      "           'activations_checkpoint_method': None,\n",
      "           'activations_checkpoint_num_layers': None,\n",
      "           'apex_transformer_log_level': 30,\n",
      "           'apply_query_key_layer_scaling': False,\n",
      "           'attention_dropout': 0.1,\n",
      "           'attention_type': 'multihead',\n",
      "           'batch_p2p_comm': True,\n",
      "           'bias': True,\n",
      "           'bias_activation_fusion': True,\n",
      "           'bias_dropout_add_fusion': True,\n",
      "           'data': {'data_impl': 'mmap',\n",
      "                    'data_prefix': '???',\n",
      "                    'dataloader_type': 'single',\n",
      "                    'eod_mask_loss': False,\n",
      "                    'exchange_indices_distributed': False,\n",
      "                    'index_mapping_dir': None,\n",
      "                    'no_seqlen_plus_one_input_tokens': False,\n",
      "                    'num_workers': 2,\n",
      "                    'pad_samples_to_global_batch_size': False,\n",
      "                    'reset_attention_mask': False,\n",
      "                    'reset_position_ids': False,\n",
      "                    'seq_length': '${model.encoder_seq_length}',\n",
      "                    'shuffle_documents': True,\n",
      "                    'skip_warmup': True,\n",
      "                    'splits_string': '900,50,50',\n",
      "                    'validation_drop_last': True},\n",
      "           'do_layer_norm_weight_decay': False,\n",
      "           'encoder_seq_length': 512,\n",
      "           'ffn_dropout': 0.0,\n",
      "           'ffn_hidden_size': 3072,\n",
      "           'fp16_lm_cross_entropy': False,\n",
      "           'fp32_residual_connection': False,\n",
      "           'fp8': False,\n",
      "           'fp8_amax_compute_algo': 'most_recent',\n",
      "           'fp8_amax_history_len': 1,\n",
      "           'fp8_e4m3': False,\n",
      "           'fp8_hybrid': False,\n",
      "           'fp8_interval': 1,\n",
      "           'fp8_margin': 0,\n",
      "           'gc_interval': 0,\n",
      "           'get_attention_mask_from_fusion': True,\n",
      "           'global_batch_size': 8,\n",
      "           'grad_allreduce_chunk_size_mb': 125,\n",
      "           'grad_div_ar_fusion': True,\n",
      "           'gradient_accumulation_fusion': False,\n",
      "           'gradient_as_bucket_view': True,\n",
      "           'headscale': False,\n",
      "           'hidden_dropout': 0.1,\n",
      "           'hidden_size': 768,\n",
      "           'hysteresis': 2,\n",
      "           'init_method_std': 0.02,\n",
      "           'kv_channels': None,\n",
      "           'layernorm_epsilon': 1e-05,\n",
      "           'make_vocab_size_divisible_by': 128,\n",
      "           'masked_softmax_fusion': True,\n",
      "           'max_position_embeddings': '${.encoder_seq_length}',\n",
      "           'megatron_amp_O2': False,\n",
      "           'micro_batch_size': 4,\n",
      "           'native_amp_growth_interval': 1000,\n",
      "           'native_amp_init_scale': 4294967296,\n",
      "           'normalization': 'layernorm',\n",
      "           'normalize_attention_scores': True,\n",
      "           'nsys_profile': {'enabled': False,\n",
      "                            'end_step': 10,\n",
      "                            'gen_shape': False,\n",
      "                            'ranks': [0],\n",
      "                            'start_step': 10},\n",
      "           'num_attention_heads': 12,\n",
      "           'num_layers': 12,\n",
      "           'num_micro_batches_with_partial_activation_checkpoints': None,\n",
      "           'onnx_safe': False,\n",
      "           'openai_gelu': False,\n",
      "           'optim': {'dampening': 0.0,\n",
      "                     'lr': 0.02,\n",
      "                     'momentum': 0.0,\n",
      "                     'sched': {'constant_steps': 50000,\n",
      "                               'min_lr': 2e-05,\n",
      "                               'name': 'CosineAnnealing',\n",
      "                               'warmup_steps': 500},\n",
      "                     'weight_decay': 0.2},\n",
      "           'overlap_p2p_comm': False,\n",
      "           'persist_layer_norm': True,\n",
      "           'pipeline_model_parallel_size': 1,\n",
      "           'position_embedding_type': 'learned_absolute',\n",
      "           'post_process': True,\n",
      "           'pre_process': True,\n",
      "           'rampup_batch_size': None,\n",
      "           'reduce_amax': True,\n",
      "           'resume_from_checkpoint': None,\n",
      "           'rotary_percentage': 1.0,\n",
      "           'seed': 1234,\n",
      "           'sequence_parallel': False,\n",
      "           'share_embeddings_and_output_weights': True,\n",
      "           'sync_batch_comm': False,\n",
      "           'tensor_model_parallel_size': 1,\n",
      "           'tokenizer': {'delimiter': None,\n",
      "                         'library': 'megatron',\n",
      "                         'merge_file': None,\n",
      "                         'model': None,\n",
      "                         'sentencepiece_legacy': False,\n",
      "                         'type': 'GPT2BPETokenizer',\n",
      "                         'vocab_file': None},\n",
      "           'transformer_block_type': 'pre_ln',\n",
      "           'transformer_engine': False,\n",
      "           'ub_tp_comm_overlap': False,\n",
      "           'ub_tp_comm_overlap_cfg': None,\n",
      "           'use_cpu_initialization': False,\n",
      "           'use_emha': False,\n",
      "           'use_flash_attention': False,\n",
      "           'use_scaled_init_method': True,\n",
      "           'virtual_pipeline_model_parallel_size': None},\n",
      " 'name': 'megatron_gpt',\n",
      " 'restore_from_path': None,\n",
      " 'trainer': {'accelerator': 'gpu',\n",
      "             'accumulate_grad_batches': 1,\n",
      "             'benchmark': False,\n",
      "             'devices': 1,\n",
      "             'enable_checkpointing': False,\n",
      "             'enable_model_summary': False,\n",
      "             'gradient_clip_val': 1.0,\n",
      "             'limit_test_batches': 500,\n",
      "             'limit_val_batches': 50,\n",
      "             'log_every_n_steps': 10,\n",
      "             'logger': False,\n",
      "             'max_epochs': -1,\n",
      "             'max_steps': 100000,\n",
      "             'num_nodes': 1,\n",
      "             'precision': 16,\n",
      "             'replace_sampler_ddp': False,\n",
      "             'val_check_interval': 100}}\n"
     ]
    }
   ],
   "source": [
    "cfg = builder.compose(task='pretraining')\n",
    "pprint(OmegaConf.to_container(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38741c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model=gpt',\n",
       " 'optim@model.optim=sgd',\n",
       " 'model.optim.lr=0.02',\n",
       " 'model.optim.weight_decay=0.2']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.overrides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffaba3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
