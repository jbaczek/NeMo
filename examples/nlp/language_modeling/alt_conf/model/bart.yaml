defaults:
  - _self_
  - tokenizer: bart_tokenizer
  - .@encoder: megatron_model_base_config
  - .@decoder: megatron_model_base_config
  - override /lr_scheduler@optim.sched: warmup_annealing

# WIP: Dummy target. Will be poped out of a config before instantiation not to break anything
_target_: nemo.collections.nlp.models.language_modeling.megatron_bart_model.MegatronBARTModel

# model parallelism 
micro_batch_size: 4
global_batch_size: 8 # will use more micro batches to reach global batch size
tensor_model_parallel_size: 1
pipeline_model_parallel_size: 1
resume_from_checkpoint: null # manually set the checkpoint file to load from
pipeline_model_parallel_split_rank: 0 # rank at which decoder starts.

# model architecture
make_vocab_size_divisible_by: 128 # Pad the vocab size to be divisible by this value for computation efficiency.

megatron_amp_O2: False # use AMP with O2 style mixed precision instead of native amp on-the-fly weight autocasting.
grad_allreduce_chunk_size_mb: 125
grad_div_ar_fusion: True # Fuse grad division into torch.distributed.all_reduce
gradient_as_bucket_view: True # Allocate gradients in a contiguous bucket to save memory (less fragmentation and buffer memory)
hysteresis: 2 # Gradient scale hysteresis

seq_length: 512
max_position_embeddings: ${.seq_length}

# weight init
embedding_init_method_std: 0.02 # Standard deviation of the zero mean normal distribution used for weight initialization.')

# embedding dropout
embedding_dropout: 0.1

# embedding sharing
share_token_embeddings: True # If True share encoder/decoder embeddings
share_decoder_tokens_head_embeddings: True # If True share decoder embeddings and decoder projection to logits

# token head
tokens_head_bias: True

# precision
native_amp_init_scale: 4294967296 # 2 ** 32
native_amp_growth_interval: 1000
fp16_lm_cross_entropy: False # Move the cross entropy unreduced loss calculation for lm head to fp16

# miscellaneous
seed: 1234
use_cpu_initialization: False # Init weights on the CPU (slow for large models)
apex_transformer_log_level: 30 # Python logging level displays logs with severity greater than or equal to this